"""
link_checker.py
==============================
Dieses Modul pr√ºft URLs effizient und parallel auf Erreichbarkeit.
Einsetzbar, um tote Links (404, SSL-Fehler, DNS-Fehler usw.) im Crawling
automatisch zu erkennen.

Zwei Kernfunktionen:
1) check_link_single()  ‚Äì pr√ºft eine einzelne URL
2) check_links_bounded() ‚Äì pr√ºft mehrere URLs mit begrenzter Parallelit√§t
"""

import aiohttp
import asyncio


# --------------------------------------------------------------------
# üîç EINZELNE LINKPR√úFUNG
# --------------------------------------------------------------------
async def check_link_single(url: str, session: aiohttp.ClientSession) -> dict:
    """
    Pr√ºft eine einzelne URL asynchron und gibt HTTP-Status oder Fehler zur√ºck.

    R√ºckgabeformat:
        {"url": <str>, "status": <int oder Fehlertext>}

    Ablauf:
    - GET-Request mit 10s Timeout
    - HTTP-Status wird zur√ºckgegeben (z. B. 200, 404)
    - Bei Fehlern (Timeout, DNS-Error, SSL-Error ‚Ä¶) wird ein Text wie
      "ERROR [TimeoutError] ..." erzeugt
    """
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
            print(f"üîó Link gepr√ºft: {url} ‚Üí Status {resp.status}")
            return {"url": url, "status": resp.status}

    except Exception as e:
        # Fehlertext konsistent erzeugen
        error_type = type(e).__name__
        error_text = str(e) if str(e) else repr(e)
        error_msg = f"ERROR [{error_type}] {error_text}"
        print(f"‚ö†Ô∏è Fehler beim Pr√ºfen von {url}: {error_msg}")
        return {"url": url, "status": error_msg}


# --------------------------------------------------------------------
# ‚öôÔ∏è PARALLELE LINKPR√úFUNG (Gesteuert √ºber Semaphore)
# --------------------------------------------------------------------
async def check_links_bounded(urls: list, max_concurrent: int = 10) -> list:
    """
    Pr√ºft mehrere URLs parallel, aber mit kontrollierter Obergrenze
    gleichzeitiger Requests (via Semaphore).

    Vorteile:
    - Keine √úberlastung von Zielservern
    - Hohe Geschwindigkeit durch parallele Ausf√ºhrung
    """
    semaphore = asyncio.Semaphore(max_concurrent)

    async with aiohttp.ClientSession() as session:

        # Jede einzelne Pr√ºfung wartet auf freie Kapazit√§t der Semaphore
        async def bounded(link):
            async with semaphore:
                return await check_link_single(link, session)

        # Starte alle Pr√ºfungen gleichzeitig
        results = await asyncio.gather(*(bounded(url) for url in urls))
        return results
